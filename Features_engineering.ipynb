{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "data = pd.read_csv(\"../data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS = '../data/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDINGS, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import operator\n",
    "\n",
    "def build_dictionary(questions):\n",
    "    d = {}\n",
    "    for sentence in tqdm(questions):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                d[word] += 1\n",
    "            except KeyError:\n",
    "                d[word] = 1\n",
    "    return d\n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    covered_word_count = 0\n",
    "    oov_word_count = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            covered_word_count += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            oov_word_count += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(covered_word_count / (covered_word_count + oov_word_count)))\n",
    "    return sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium'}\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def correct_mispelling(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "to_remove = ['a','to','of','and']\n",
    "def remove_stop_words(x):\n",
    "    return [word for word in x if word not in to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306122 rows loaded...\n",
      "columns are: ['qid', 'question_text', 'target']\n",
      "93.81% of sincere questions\n",
      "6.19% of insincere questions\n"
     ]
    }
   ],
   "source": [
    "def print_datasets_info(df):\n",
    "    print(\"{} rows loaded...\".format(df.shape[0]))\n",
    "    print(\"columns are: {}\".format(list(df.columns)))\n",
    "    print(\"{0:.2f}% of sincere questions\".format(len(df[df['target'] == 0])*100/df.shape[0]))\n",
    "    print(\"{0:.2f}% of insincere questions\".format(len(df[df['target'] == 1])*100/df.shape[0]))\n",
    "print_datasets_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13061 rows loaded...\n",
      "columns are: ['qid', 'question_text', 'target']\n",
      "93.93% of sincere questions\n",
      "6.07% of insincere questions\n"
     ]
    }
   ],
   "source": [
    "reducted_data = data.sample(frac=0.01)\n",
    "print_datasets_info(reducted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and tokenizing questions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd7f2bf1b5647b7a30e8ee0ea0c6806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13061), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa57cfd93dca47c8ad74329e3c97dd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13061), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1ca19bd3bc4709bed086f8b90a7e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13061), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5199015fa3f041f4a1133eff4c6289c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13061), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removing stop words...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85a73394108462c9850d1f3bc2d3b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13061), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building dictionary...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ece8040288b41cab705f853d34b85a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13061), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking coverage...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac41b5298ae42a48cbf24f5ef2eb2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19143), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found embeddings for 92.79% of vocab\n",
      "Found embeddings for  99.01% of all text\n"
     ]
    }
   ],
   "source": [
    "#dictionary and text coverage with cleaned text\n",
    "print(\"Cleaning and tokenizing questions...\")\n",
    "cleaned_questions = reducted_data['question_text']\\\n",
    "    .progress_apply(lambda x: clean_text(x))\\\n",
    "    .progress_apply(lambda x: clean_numbers(x))\\\n",
    "    .progress_apply(lambda x: correct_mispelling(x))\\\n",
    "    .progress_apply(lambda x: word_tokenize(x))\n",
    "print(\"Removing stop words...\")\n",
    "cleaned_questions = [remove_stop_words(sentence) for sentence in tqdm(cleaned_questions)]\n",
    "print(\"Building dictionary...\")\n",
    "dictionary = build_dictionary(cleaned_questions)\n",
    "print(\"Checking coverage...\")\n",
    "out_of_dict = check_coverage(dictionary, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting sentence in embedded vectors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c2cc80d9db4d2ba18c9769ad2b4bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13061), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def text_to_array(text, max_text_len = 30):\n",
    "    empyt_emb = np.zeros(300)\n",
    "    embeds = [embeddings_index[x] if x in embeddings_index else empyt_emb for x in text[:max_text_len]]\n",
    "    embeds+= [empyt_emb] * (max_text_len - len(embeds))\n",
    "    return np.array(embeds)\n",
    "\n",
    "print(\"Converting sentence in embedded vectors\")\n",
    "X = np.array([text_to_array(sentence) for sentence in tqdm(cleaned_questions)])\n",
    "y = reducted_data['target']\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english')\n",
    "\n",
    "def textPreProcessing(text):\n",
    "    def tokenize_text(text):\n",
    "        return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "    def remove_special_characters(text, characters=string.punctuation.replace('-', '')):\n",
    "        tokens = tokenize_text(text)\n",
    "        pattern = re.compile('[{}]'.format(re.escape(characters)))\n",
    "        return ' '.join(filter(None, [pattern.sub('', t) for t in tokens]))\n",
    "\n",
    "    def stem_text(text, stemmer=default_stemmer):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "    def remove_stopwords(text, stop_words=default_stopwords, pos=True):\n",
    "        tokens = [w for w in tokenize_text(text) if w[:w.rfind('/')] not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def addPOS(text):\n",
    "        tokens = [\"{}/{}\".format(word[0], word[1]) for word in nltk.pos_tag(tokenize_text(text))]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    text = remove_special_characters(text)\n",
    "    text = text.lower() # lowercase\n",
    "    text = addPOS(text) #add part-of-speech\n",
    "    text = remove_stopwords(text) # remove stopwords\n",
    "    text = text.strip(' ') # strip whitespaces\n",
    "    text = stem_text(text) # stemming\n",
    "    #text.strip(' ') #strip whitespaces again?\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$ boyfriend/nn says/vbz loves/vbz crush/nn another/dt girl/nn'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textPreProcessing(X.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from pattern.en import parse, Sentence, parse\n",
    "from pattern.en import modality, mood\n",
    "\n",
    "def calculate_features(X):    \n",
    "    def polarity_subj(text):\n",
    "        t = TextBlob(text)\n",
    "        return t.sentiment.polarity, t.sentiment.subjectivity\n",
    "    \n",
    "    def feature_mood(text):\n",
    "        print(text)\n",
    "        t = parse(text, lemmata=True)\n",
    "        t = Sentence(t)\n",
    "        return mood(t)\n",
    "    \n",
    "    def build_features_dict(x):\n",
    "        d = {'polarity': polarity_subj(x)[0], \n",
    "             'subjectivity': polarity_subj(x)[1],\n",
    "             'indicative': 0,\n",
    "             'imperative': 0,\n",
    "             'conditional': 0,\n",
    "             'subjunctive': 0}\n",
    "        d[mood(x).lower()] = 1\n",
    "        return d\n",
    "    \n",
    "    v = DictVectorizer(sparse=True)\n",
    "    return v.fit_transform([build_features_dict(x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "features_vect = calculate_features(X)\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                             preprocessor=textPreProcessing, max_features=10000)\n",
    "dict_vect = vectorizer.fit_transform(X)\n",
    "\n",
    "X_vect = hstack([dict_vect, features_vect])\n",
    "#X_vect = calculate_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.neural_network import MLPClassifier\n",
    "#clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(30,), random_state=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.fit(training_vectors, y_train[:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13061x5006 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 194485 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm = svm.LinearSVC(C=1, max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   5.0s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   6.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   6.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   5.4s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   29.8s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "reshaped = X.reshape(len(X),300*30)\n",
    "scores = cross_validate(svm, reshaped, y, cv=5, verbose=2, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST RESULT OF 5-FOLD CV: \n",
      "\tAccuracy: 0.9318\n",
      "\tPrecision: 0.431\n",
      "\tRecall: 0.377\n",
      "\tF1: 0.4017\n"
     ]
    }
   ],
   "source": [
    "def print_cv_results(scores):\n",
    "    print(\"TEST RESULT OF {}-FOLD CV: \".format(len(scores['fit_time'])))\n",
    "    print(\"\\tAccuracy: {:.4}\".format(scores['test_accuracy'].mean()))\n",
    "    print(\"\\tPrecision: {:.4}\".format(scores['test_precision'].mean()))\n",
    "    print(\"\\tRecall: {:.4}\".format(scores['test_recall'].mean()))\n",
    "    print(\"\\tF1: {:.4}\".format(scores['test_f1'].mean()))\n",
    "    \n",
    "print_cv_results(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
