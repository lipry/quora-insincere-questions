{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from autocorrect import spell\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306122 rows loaded...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1017"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading data\n",
    "data = pd.read_csv(\"../data/train.csv\")\n",
    "print(\"{} rows loaded...\".format(data.shape[0]))\n",
    "data.question_text.map(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS = '../data/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDINGS, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index['is'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import operator\n",
    "\n",
    "def build_dictionary(questions):\n",
    "    d = {}\n",
    "    for sentence in tqdm(questions):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                d[word] += 1\n",
    "            except KeyError:\n",
    "                d[word] = 1\n",
    "    return d\n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    covered_word_count = 0\n",
    "    oov_word_count = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            covered_word_count += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            oov_word_count += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(covered_word_count / (covered_word_count + oov_word_count)))\n",
    "    return sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium'}\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def correct_mispelling(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "to_remove = ['a','to','of','and']\n",
    "def remove_stop_words(x):\n",
    "    return [word for word in x if word not in to_remove]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#dictionary and text coverage without cleaning text\n",
    "print(\"Tokenizing questions...\")\n",
    "questions = data['question_text'].progress_apply(lambda x: word_tokenize(x))\n",
    "print(\"Building dictionary...\")\n",
    "dictionary = build_dictionary(questions)\n",
    "print(\"Checking coverage...\")\n",
    "out_of_dict = check_coverage(dictionary, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and tokenizing questions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9aa75173d254b3f9a3090e436eaa07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f13958eab646979880229f2d258c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efba7f9f7814cec9afe0a9aeb301d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874594c7379747bbaa26e3c4e1b688d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removing stop words...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae99c181858449187baf847f22823e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building dictionary...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6a8ae601a24c64966f12e2780604fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking coverage...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac36cd4f460c4abe80114b7eac6c845f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=238750), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found embeddings for 61.06% of vocab\n",
      "Found embeddings for  99.00% of all text\n"
     ]
    }
   ],
   "source": [
    "#dictionary and text coverage with cleaned text\n",
    "print(\"Cleaning and tokenizing questions...\")\n",
    "cleaned_questions = data['question_text']\\\n",
    "    .progress_apply(lambda x: clean_text(x))\\\n",
    "    .progress_apply(lambda x: clean_numbers(x))\\\n",
    "    .progress_apply(lambda x: correct_mispelling(x))\\\n",
    "    .progress_apply(lambda x: word_tokenize(x))\n",
    "print(\"Removing stop words...\")\n",
    "cleaned_questions = [remove_stop_words(sentence) for sentence in tqdm(cleaned_questions)]\n",
    "print(\"Building dictionary...\")\n",
    "dictionary = build_dictionary(cleaned_questions)\n",
    "print(\"Checking coverage...\")\n",
    "out_of_dict = check_coverage(dictionary, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bitcoin', 987),\n",
       " ('‘', 874),\n",
       " ('Quorans', 858),\n",
       " ('cryptocurrency', 822),\n",
       " ('Snapchat', 807),\n",
       " ('btech', 632),\n",
       " ('Brexit', 493),\n",
       " ('cryptocurrencies', 481),\n",
       " ('blockchain', 474),\n",
       " ('behaviour', 468)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_of_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_array(text, max_text_len = 30):\n",
    "    empyt_emb = np.zeros(300)\n",
    "    embeds = [embeddings_index[x] if x in embeddings_index else empyt_emb for x in text[:max_text_len]]\n",
    "    embeds+= [empyt_emb] * (max_text_len - len(embeds))\n",
    "    return np.array(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generator for Keras models\n",
    "def data_generator(X, Y, batch_size=128):\n",
    "    n_batches = len(X) // batch_size\n",
    "    while True:\n",
    "        for index in range(n_batches):\n",
    "            questions = X[index*batch_size:(index+1)*batch_size]\n",
    "            vectorized = [text_to_array(q) for q in questions]\n",
    "            yield np.array(vectorized), np.array(Y[index*batch_size:(index+1)*batch_size])\n",
    "\n",
    "def data_generator_test(X, batch_size=128):\n",
    "    n_batches = len(X) // batch_size\n",
    "    for index in range(n_batches):\n",
    "        questions = X[index*batch_size:(index+1)*batch_size]\n",
    "        vectorized = [text_to_array(q) for q in questions]\n",
    "        yield np.array(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data in training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(cleaned_questions, data['target'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/engine/training.py:2087: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9603"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/engine/training.py:2330: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 309s 309ms/step - loss: 0.1257 - acc: 0.9603 - val_loss: 0.1942 - val_acc: 0.9203\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 307s 307ms/step - loss: 0.0844 - acc: 0.9724 - val_loss: 0.1702 - val_acc: 0.9313\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 318s 318ms/step - loss: 0.0849 - acc: 0.9719 - val_loss: 0.1907 - val_acc: 0.9297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29bb078d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Bidirectional, Dropout, Conv1D, MaxPooling1D\n",
    "#Building Convolutional Neural Network\n",
    "def build_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Dropout(0.2, input_shape=(30, 300)))\n",
    "    model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(LSTM(100))\n",
    "    model_conv.add(Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "training_generator = data_generator(X_train, y_train)\n",
    "validation_generator = data_generator(X_val, y_val)\n",
    "model.fit_generator(training_generator, \n",
    "                    epochs = 3,\n",
    "                    steps_per_epoch=1000,\n",
    "                    verbose=True,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps = 10,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6875b0e38d40dc8fbb4e96e88a257b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56370), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66311ff3ce544b91a679441a079f5815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56370), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9447f96a7de41989433a12c18512721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56370), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da792a3473644c0806e8e4a24c7b3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56370), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4325027a5941f1b03f630aae63908e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56370), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Trying to predict\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "cleaned_test_set = test_df['question_text']\\\n",
    "    .progress_apply(lambda x: clean_text(x))\\\n",
    "    .progress_apply(lambda x: clean_numbers(x))\\\n",
    "    .progress_apply(lambda x: correct_mispelling(x))\\\n",
    "    .progress_apply(lambda x: word_tokenize(x))\n",
    "cleaned_test_set = [remove_stop_words(sentence) for sentence in tqdm(cleaned_test_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879/1879 [==============================] - 17s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56370"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator = data_generator_test(cleaned_test_set, batch_size=30)\n",
    "predictions = model.predict_generator(test_generator, steps=56370//30, verbose=True)\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te = (np.array(predictions.T) > 0.5).astype(np.int)\n",
    "submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te[0]})\n",
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
